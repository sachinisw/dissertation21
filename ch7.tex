\chapter{Concluding Remarks}
In this dissertation we introduce the Intervention Problem and discuss three solutions addressing different properties of the problem.
Intervention is important when an agent or a human user is executing tasks in an unfamiliar environment and unknown facts about the environment may cause the tasks to have unintended, perhaps dangerous consequences.
It is also important that upon recognizing that intervention is needed, there is a system in place to guide the agent (or the human user) toward the goal, while avoiding the undesirable consequences.
Thus, we propose intervention as a utility for online assistive agents and safety critical decision making for human users.
The underlying assumption about the agent's (or the human user's) environment is that it can be modeled as a state transition system that consists of actions and states.
Some states in the environment are undesirable and the agent (or the human user) does not have the ability to recognize them.
An observer monitors what the agent is doing and wants to help the agent avoid the undesirable state.

\begin{figure}[tpb]
 \centering{\includegraphics[width=\columnwidth]{img/summary.pdf}}
   \caption{The Intervention Framework}
\label{fig:summary}
\end{figure}

\section{The Intervention Recognition Problem}
As illustrated in Figure~\ref{fig:summary}, we model the Intervention Problem consisting of two sub problems.
The first sub problem, known as \textbf{The Undesirable State Recognition Process}, the observer automatically detects that an undesirable state is developing.
We present three intervention models each considering the properties: (1) actors in the environment, (2) goals hidden to the observer, (3) types of observations (4) noise in observations and (5) handling intervention recovery.
In all three models, a key objective we want to address during the recognition process is ensuring the safety while allowing some freedom for the agent.

\subsection{Intervention by Recognizing Actions Enabling Multiple Undesirable Consequences}
Using the cyber security domain as the motivation, we model an environment consisting of three agents: the user, the attacker and the observer.
The observer wants to help the user reach a hidden desirable goal, while avoiding multiple undesirable states enabled by an attacker.
Because the undesirable states are hidden to the user, he becomes an unwitting accomplice to security breaches.
The observations that are used to make the intervention decision have missing and extraneous actions.
The intervention recovery process is to simply block the recognized undesirable action by issuing an alert.
Our approach views the intervention decision as a multi-factor decision problem modeled with three domain-independent metrics: \textbf{certainty}, \textbf{timeliness} and \textbf{desirability}.
The observer projects the possible plans to reach the undesirable states using automated planning and traces back to find actions that are critical to the occurrence of the undesirable states.
We simulate the trade-off between the safety and freedom of the agent (or the human user) when selecting actions requiring intervention by factoring in the domain-independent metrics with varying degrees of importance.
Our experiments find that the certainty and desirability  metrics deal well to the extraneous actions in the observation trace, while the timeliness metric is sensitive to the missing actions.

\subsection{Intervention as Classical Planning}
Using the benchmark planning domains, we model scenarios where the user the observer and an optional competitor attempt to achieve different goals in the same environment.
A key difference in this form of intervention is that the observer is aware of  the user's desirable goal and the undesirable state.
Partial knowledge about the domain precludes the user from reaching his own goal and avoid facilitating the goal pursued by the competitor.
In another scenario, the user may also unwittingly reach a hidden undesirable state on his own.
The Intervention as Planning model allows the observer to intervene and guide the user towards his own goal while avoiding undesirable outcomes (i.e., the competitor's goal or hidden undesirable states) or frustration.
The observer projects the possible plans to reach the undesirable and desirable states exactly and approximately using automated planning.
To decide if intervention is necessary, the observer analyzes the plan suffixes leading to the user's desirable goal and the undesirable state.
In one method, the observer uses the Intervention Graph, a data structure that represents the plan space to derive domain-independent metrics to analyze the suffixes of partially executed plans.
In another method, the observer uses plan distance metrics to measure the differences between a projected plan hypothesis to the safe and the unsafe plans generated by an automated planner.
In both cases, the observer uses the metrics to learn the differences between the safe and the unsafe suffixes (using off-the-shelf classification algorithms) and balance specific unsafe actions with those that are necessary for allowing the user some freedom.
We compare the two intervention models to intervention by plan recognition for benchmark planning domains and show that our proposed approaches outperform three existing plan recognition algorithms.

\subsection{Human-aware Intervention}
In the two previous intervention models, we assume the user is an agent(s) using automated planners to generate the plans to achieve the desirable (and albeit by mistake, the undesirable states).
When human users plan, especially for a cognitively taxing task, they do not have the ability to use heuristics to search for the best solutions.
They often make mistakes and spend time exploring the search space of a planning problem, which in turn may cause them to accidentally reach undesirable states.
To the Intervention Problem, what this means is that deciding to intervene by analyzing plan suffixes generated by an automated planner is not feasible when human users plan.
Using the Rush Hour puzzle as the case study, we conduct a human subject experiment to analyze how human users solve a cognitively engaging puzzle as a planning task.
We use the findings of this study to develop a Human-aware Intervention model combining automated planning and machine learning, where the observer must decide in real time whether to intervene for human user using a domain specific feature set more appropriate for human behavior.
We show that the Human-aware Intervention model outperforms in recognizing undesirable outcomes in advance, compared to the existing plan recognition algorithms.


\section{The Intervention Recovery Problem}
The second sub problem, known as \textbf{The Recovery Process}, we go beyond the typical preventive measures to help the human user recover from intervention and interactively guide him toward safety using a new feedback technique called the \textbf{Interactive Human-aware Intervention}.
In this planning task, the environment contains the human user and the observer who recognizes intervention using the Human-aware Intervention model.
The observer is aware of the user's goal and also the undesirable states that are hidden from the user.
We introduce a forbidden vehicle, which does not have to be moved to solve the Rush Hour planning task.
Since the forbidden vehicle move is unnecessary, if the user moves that vehicle it indicates that the user has moved away from the solution to the planning task (the solution an automated planner would produce) and as a result needs intervention.
We design an interactive feedback process where the observer, upon recognizing that the human user needs help, suggests information about the search space of the planning task as hints for the user.
The hints are: \textbf{the number of remaining moves}, \textbf{the next best move}, \textbf{the vehicles that must be moved} and \textbf{restart the planning task}.
We evaluate the effectiveness of the Interactive Human-aware Intervention using the evaluation metrics that combine the cost optimal solutions produced by automated planners and also the planning landmarks:
\begin{enumerate}
\item The number of moves in the solution
\item The cost difference compared to the cost optimal solution
\item The latest time a fact landmark is eventually achieved
\item The number of times a fact landmark is lost and regained
\end{enumerate}
When considering the number of moves as the evaluation metric, we observe that for some Rush Hour planning tasks, using the Interactive Human-aware Intervention reduces the solution length. 
However, this effect is not statistically significant compared to human users solving the same planning tasks without the Interactive Human-aware Intervention.
For the evaluation metrics 2, 3, and 4, we observe a similar effect where for some Rush Hour planning tasks, using the Interactive Human-aware Intervention pushes the user closer to the optimal.
However, here too, the effects are not statistically significant.


\section{Future Work}
Finally, we discuss possible extensions for our study of the Intervention problem.

\subsection{Exploring Different Models of the Environment}
The planning model is concerned with the Intervention Recognition sub problem, which requires some way to model how the agents (human or artificial) in the environment execute actions and how the actions modify the state.
The intervention recognition models in our current implementations assume STRIPS planning models, where
the world state is represented by a set of ground atomic literals and a plan is produced by chaining a sequence of ground actions, where each action has pre and post conditions.
Another planning model that is useful, particularly in describing complex tasks is the Hierarchical Task Networks (HTN) \cite{erol1995htn}.
The HTN planning model decomposes the events that take place in the environment into tasks and sub-tasks.
The HTN planner receives a set of tasks to be performed and a plan is produced by repeatedly decomposing tasks into smaller and smaller sub-tasks until primitive tasks that can not be further decomposed are found.
For intervention to be meaningful the scenario modeled in the planning domain must be complex, where one task may have several ways to be subverted (maliciously or by constraints in the environment).
An example for a complex domain is Search and Rescue (SaR).
In a SaR environment actors with specialized capabilities (e.g., police, victims, ambulance units, mission commanders) will collaborate to complete a rescue activity. 
All actors do not have full domain knowledge and as a result their actions may prevent other actors from reaching their own goals. 
SaR domain allows us to address situation-specific intervention where the intervention process need to account for different goals of agents collaborating to achieve a common higher-level goal.

\subsection{Domain Abstraction Techniques to Explain Intervention}
This issue focuses on the Intervention Recovery sub problem.
The Interactive Human-aware Intervention model we have presented in this dissertation is different from the existing explanation types in the Explainable Artificial Intelligence Planning (XAIP), where the explainee can ask why-questions like ``Why did you do action A?'', ``Why didn't you do something else that I would have done?'' and ``Why can't you do that?''.
For these questions, the XAIP literature suggests extracting contrastive or selective explanations from the planning domain to give an answer describing the cause and the effect relationship of the event in question \cite{miller2017}.
Our intention with the interactive Human-aware Intervention model is to guide the human user toward the solution, without giving away the solution directly.
However, in complex planning domains the causal explanation model is also useful to help the user understand the intervention, by explaining what caused the intervention.
The downside of complex planning domains, as required for intervention is that the plans used in the intervention decision process may be too difficult to understand for the human user.
For example, in the cyber-security domain, many actions are possible, the human user only performs a limited set of actions.
Even fewer number of actions trigger the security breaches.
By simplifying the model the planner may find shorter plans, which in turn may result in simpler causal models for producing explanations.
It is possible to explore the use of \textbf{macro-actions}: a domain abstraction technique to simplify the causal models and plans generated from complex domains \cite{macro207}.

\subsection{Ensuring the Longevity of the Interactive Intervention Models}
Intervention models designed for human user-agent collaborations must be robust to phenomena that have not been explicitly modeled.
Even if the task is unfamiliar in the beginning, human users may learn over time, while interacting with the system
The actor's goals may not be explicit, which requires the intervention agent to incorporate goal recognition as a prerequisite for the intervention model.
Explanations for intervention must be catered toward human users having different expertise levels.
Human users, and even artificial agents operating with partial information may make mistakes collaborating in a complex domain.
Intervention can also be used to help the actors learn over time.
It will be interesting to explore how the domain model itself can be learned from observations to account for human user's gaining experience and learning over time so that the longevity of the intervention model can be ensured.
Maintaining portfolios of explainable models combining explainable black-box learning models and causal explainable models can be helpful in producing explanations catered toward end users with different domain levels of domain expertise.


\begin{center}
\textbf{\textsc{the end}}
\end{center}